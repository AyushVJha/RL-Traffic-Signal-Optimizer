# ğŸš¦ RL-Powered Traffic Signal Optimizer

A reinforcement learning project that trains an agent to dynamically control a 4-way traffic signal using Deep Q-Learning. The goal is to minimize vehicle congestion and waiting time using intelligent policy learning.

---

## ğŸ“Š Project Highlights

| Feature                        | Description                                                   |
|-------------------------------|---------------------------------------------------------------|
| âœ… Custom `gym.Env`           | Designed from scratch for 4-lane traffic simulation           |
| âœ… RL Algorithm                | Deep Q-Network (DQN) via Stable-Baselines3                    |
| âœ… Interface                   | Streamlit demo to simulate agent decisions in real-time       |
| âœ… Visualization               | Reward-vs-episode curve plotted using Matplotlib              |
| âœ… Real-time inference         | Runs inference on trained agent in CLI and web UI             |
| âœ… Resume-Ready Metrics        | 200 episodes, reward ~-600, steps/episode: 100                |

---

## ğŸ¯ Problem Statement

Traditional traffic signals use fixed timers, which fail to adapt to real-time conditions. This project simulates an **adaptive RL-based signal controller** that:
- Observes the traffic queue on all four sides
- Decides whether to keep Northâ€“South or Eastâ€“West signal green
- Learns to minimize congestion and queue buildup

---

## ğŸ§  Model & Training Stats

- **Episodes Trained**: 200
- **Average Reward**: `-604` per 100-step episode (improved from ~-800 baseline)
- **FPS**: ~2600
- **Policy**: Fully trained DQN with epsilon decay to 0.05
- **Model Saved**: âœ… Yes (`models/dqn_traffic_model.zip`)

---

## ğŸ“¦ Tech Stack

- Python ğŸ
- [Stable-Baselines3](https://github.com/DLR-RM/stable-baselines3) (DQN)
- OpenAI Gym (Custom Env)
- NumPy, Matplotlib
- Streamlit (Web UI)

---

## ğŸš€ How to Run

### 1. Install Requirements
```bash
pip install -r requirements.txt
```

### 2. Train the Agent
```bash
python train.py
```

### 3. Evaluate Agent Performance
```bash
python evaluate.py
```

### 4. Plot Reward Curve
```bash
python notebooks/plot_rewards.py
```

### 5. Run the Streamlit Demo
```bash
streamlit run streamlit_app/app.py
```

---

## ğŸ“ˆ Reward vs. Episode Plot

> _Include this image if you have it_
![Reward Curve](training_reward_plot.png)

---

## ğŸ“¬ Example Output (Console)

```txt
Traffic: N:2 S:4 E:5 W:5
Action: NS Green
Traffic: N:1 S:1 E:0 W:3
...
```

---

## ğŸ“ Folder Structure

```
RL-Traffic-Signal-Optimizer/
â”œâ”€â”€ env/                   # Custom Gym environment
â”œâ”€â”€ models/                # Saved DQN model
â”œâ”€â”€ streamlit_app/         # Web interface
â”œâ”€â”€ notebooks/             # Reward visualization
â”œâ”€â”€ train.py               # RL training
â”œâ”€â”€ evaluate.py            # Run trained agent
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸ“œ License

MIT License Â© 2024

---

## ğŸ’¼ Add to Resume

> âœ… Trained a DQN agent using Stable-Baselines3 to optimize a 4-way traffic signal  
> ğŸ“‰ Achieved average episode reward of ~-600 vs static baseline  
> ğŸ§  Designed custom OpenAI Gym environment and deployed real-time demo via Streamlit

---

## ğŸ”— Links

- [SB3 Docs](https://stable-baselines3.readthedocs.io/)
- [Gym API](https://www.gymlibrary.dev/)
